{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 1\n",
    "\n",
    "Aluno: <span style=\"color:red\">Escreva seu nome aqui</span>\n",
    "\n",
    "Entrega: 16 de maio de 2024\n",
    "\n",
    "Neste projeto você vai construir um modelo de regressão sobre um dataset de notas finais de alunos do ensino fundamental em relação a parâmetros socio-ambientais dos mesmos. O dataset foi obtido de \n",
    "https://archive.ics.uci.edu/dataset/320/student+performance.\n",
    "\n",
    "## Descrição do projeto\n",
    "\n",
    "O projeto tem duas partes: modelagem inicial, e modelo com filtragem dos dados.\n",
    "\n",
    "### Fase 1 \n",
    "\n",
    "Na parte inicial, você deverá:\n",
    "\n",
    "- Realizar *feature engineering* conforme necessário\n",
    "    - Por exemplo: codificar as variáveis categóricas com `OneHotEncoding`\n",
    "    - Não remova *outliers* aqui: faremos isso na seção seguinte\n",
    "- Construir três modelos de regressão:\n",
    "    - Um modelo usando o regressor `DummyRegressor` do scikit-learn (https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html) que vai servir de \"regressor trivial\" para nós.\n",
    "    - Um modelo `Ridge`.\n",
    "    - Um outro modelo qualquer à sua escolha.\n",
    "    Como o professor é santo, os dois primeiros já estão implementados para você! Então só falta escolher o terceiro.\n",
    "- Fazer o ajuste de hiperparâmetros dos modelos. O código de exemplo já mostra como fazer o ajuste dos modelos iniciais, você especifica o grid de parâmetros de teste para o seu terceiro modelo.\n",
    "- Comparar os desempenhos de modelos de modo adequado. Estude o material em https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html para saber como fazer o teste t com compensação de correlação dos desempenhos dos modelos em um cenário de validação cruzada.\n",
    "- Escolhido o melhor modelo, treiná-lo no conjunto de treinamento e medir o desempenho no\n",
    "conjunto de teste, para estimar o desempenho de generalização do modelo.\n",
    "- Escrever suas conclusões à respeito do que foi aprendido acerta do modelo, por exemplo:\n",
    "    - Quais as consequências do desempenho do modelo final para a estimação das notas finais?\n",
    "    - Quais features são mais importantes na determinação da estimativa da nota final? Note que essa pergunta pode ou não ter resposta, dependendo das capacidades dos modelos de regressão que você escolher.\n",
    "\n",
    "### Fase 2\n",
    "\n",
    "Agora execute uma filtragem dos dados de treinamento para remover valores anormais. Por exemplo:\n",
    "\n",
    "- Remova os alunos que tiveram nota final zero\n",
    "- Remova os alunos que faltaram muito - defina você mesmo o que é \"faltar muito\"\n",
    "\n",
    "Refaça o processo da fase 1 para esses dados filtrados.\n",
    "\n",
    "### Rubrica\n",
    "\n",
    "Os itens a serem avaliados no projeto são:\n",
    "\n",
    "FEAT – Fazer feature engineering adequadamente\n",
    "\n",
    "HYPER – Para cada modelo, fez ajuste de hiperparâmetros adequadamente\n",
    "\n",
    "MODEL – Treinou e comparou adequadamente os modelos para selecionar o melhor modelo\n",
    "\n",
    "PERF – Análise de desempenho do modelo. NÃO SERÁ EXIGIDA NENHUMA PERFORMANCE ESPECÍFICA, ISTO NÃO É UMA COMPETIÇÃO DE DESEMPENHO.\n",
    "\n",
    "FILT - Executar a fase 2: filtragem de dados adequada e repetição do experimento.\n",
    "\n",
    "- I – Insuficiente\n",
    "\n",
    "    - Não entregou, ou entregou abobrinha\n",
    "\n",
    "- D – Em desenvolvimento\n",
    "\n",
    "    - Faltou um de FEAT, MODEL, PERF\n",
    "\n",
    "- C – Minimo aceitável\n",
    "\n",
    "    - Fez FEAT, MODEL, PERF\n",
    "\n",
    "- B – Esperado\n",
    "\n",
    "    - Fez FEAT, HYPER, MODEL, PERF\n",
    "\n",
    "- A – Excepcional\n",
    "\n",
    "    - Fez FEAT, HYPER, MODEL, PERF, FILT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicio do projeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura dos dados e correção de tipos de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path.cwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir: Path = DATA_DIR) -> pd.DataFrame:\n",
    "    # Read data and fix column types.\n",
    "    df = pd.read_csv(data_dir / 'data.csv')\n",
    "\n",
    "    categorical_columns = [\n",
    "        'school',\n",
    "        'sex',\n",
    "        'address',\n",
    "        'famsize',\n",
    "        'Pstatus',\n",
    "        'Mjob',\n",
    "        'Fjob',\n",
    "        'reason',\n",
    "        'guardian',\n",
    "        'schoolsup',\n",
    "        'famsup',\n",
    "        'paid',\n",
    "        'activities',\n",
    "        'nursery',\n",
    "        'higher',\n",
    "        'internet',\n",
    "        'romantic',\n",
    "    ]\n",
    "    numerical_columns = [\n",
    "        'age',\n",
    "        'absences',\n",
    "        'grade',\n",
    "    ]\n",
    "    ordinal_columns = [\n",
    "        'Medu',\n",
    "        'Fedu',\n",
    "        'traveltime',\n",
    "        'studytime',\n",
    "        'failures',\n",
    "        'famrel',\n",
    "        'freetime',\n",
    "        'goout',\n",
    "        'Dalc',\n",
    "        'Walc',\n",
    "        'health',\n",
    "    ]\n",
    "    for column_group, column_type in (\n",
    "        (categorical_columns, 'category'),\n",
    "        (numerical_columns, 'float64'),\n",
    "        (ordinal_columns, 'int64'),\n",
    "    ):\n",
    "        for column in column_group:\n",
    "            df[column] = df[column].astype(column_type)\n",
    "\n",
    "    # Split features and target, and return.\n",
    "    X = df.drop(columns=['grade']).copy()\n",
    "    y = df['grade'].copy()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualização simples dos dados\n",
    "\n",
    "Chamar isso de análise exploratória é vexatório..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_barplots(X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(12, 45))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.tight_layout(h_pad=8, w_pad=5)\n",
    "\n",
    "    for index, (column_name, column_series) in enumerate(X.items()):\n",
    "        column_series \\\n",
    "            .value_counts() \\\n",
    "            .sort_index(ascending=False) \\\n",
    "            .plot \\\n",
    "            .barh(ax=axes[index])\n",
    "        axes[index].set_title(column_name)\n",
    "\n",
    "    fig.suptitle('Barplots of Categorical and Ordinal Features', fontsize=16)\n",
    "    fig.subplots_adjust(top=0.96)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(3, 4))\n",
    "    y.plot.hist(bins=20, edgecolor='black')\n",
    "    plt.title('Histogram of Target')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_boxplots(X: pd.DataFrame, y: pd.Series) -> None:\n",
    "    df = pd.concat([X, y], axis=1)\n",
    "\n",
    "    fig, axes = plt.subplots(10, 3, figsize=(12, 45))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    fig.tight_layout(h_pad=8, w_pad=5)\n",
    "\n",
    "    for index, column in enumerate(X.columns):\n",
    "        ax = axes[index]\n",
    "        bp_objs = df.boxplot(\n",
    "            by=['school', column],\n",
    "            column='grade',\n",
    "            ax=ax,\n",
    "            rot=45,\n",
    "            fontsize=8,\n",
    "            return_type='dict',\n",
    "            patch_artist=True,\n",
    "        )\n",
    "        boxes = bp_objs['grade']['boxes']\n",
    "        box_colors = [\n",
    "            'lightgreen' if '(GP,' in tick.get_text() else 'lightblue'\n",
    "            for tick in ax.get_xticklabels()\n",
    "        ]\n",
    "        for box, color in zip(boxes, box_colors):\n",
    "            box.set_facecolor(color)\n",
    "        ax.set_title(column)\n",
    "        ax.set_xlabel('')\n",
    "    fig.suptitle('Grade distribution by feature and school', fontsize=16)\n",
    "    fig.subplots_adjust(top=0.96)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_barplots(X, y)\n",
    "make_boxplots(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação treino-teste e modelagem inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = X.select_dtypes(include=['category']).columns\n",
    "numerical_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    # O que mais pode ir aqui?\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('encoder', OneHotEncoder(drop='first')),\n",
    "])\n",
    "\n",
    "preprocessing_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_pipeline, numerical_features),\n",
    "        ('cat', cat_pipeline, categorical_features),\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessing_pipeline),\n",
    "    ('regressor', DummyRegressor(strategy='mean')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, ShuffleSplit\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Quanto maior o numero de splits, maior a significância estatística da\n",
    "# validação cruzada, mas também maior o tempo de execução.\n",
    "num_splits = 1000\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'regressor': [DummyRegressor(strategy='mean')],\n",
    "    },\n",
    "    {\n",
    "        'regressor': [Ridge()],\n",
    "        'regressor__alpha': np.logspace(-3, 3, 7),\n",
    "    },\n",
    "    # {\n",
    "    #     'regressor': [ponha seu regressor aqui],\n",
    "    #     'regressor__algumparametro': [coloque a lista de valores aqui],\n",
    "    #     'regressor__algumoutroparametro': [etc...],\n",
    "    # },\n",
    "]\n",
    "\n",
    "test_fraction = 0.2\n",
    "num_samples_total = len(y_train)\n",
    "num_samples_test = int(test_fraction * num_samples_total)\n",
    "num_samples_train = num_samples_total - num_samples_test\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    pipe,\n",
    "    param_grid,\n",
    "    cv=ShuffleSplit(\n",
    "        n_splits=num_splits,\n",
    "        test_size=num_samples_test,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    ")\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(grid.cv_results_) \\\n",
    "    .sort_values(by='rank_test_score')\n",
    "\n",
    "results_df = results_df \\\n",
    "    .set_index(\n",
    "        results_df[\"params\"] \\\n",
    "            .apply(lambda x: \"_\".join(str(val) for val in x.values()))\n",
    "    ) \\\n",
    "    .rename_axis(\"model\")\n",
    "\n",
    "model_scores = results_df.filter(regex=r\"split\\d*_test_score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scores.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_perf = model_scores.agg(['mean', 'std'], axis=1)\n",
    "mean_perf['std'] = mean_perf['std'] / np.sqrt(num_splits)\n",
    "mean_perf = mean_perf.sort_values('mean', ascending=False)\n",
    "mean_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código a seguir foi copiado de https://scikit-learn.org/stable/auto_examples/model_selection/plot_grid_search_stats.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.stats import t\n",
    "\n",
    "\n",
    "def corrected_std(differences, n_train, n_test):\n",
    "    \"\"\"Corrects standard deviation using Nadeau and Bengio's approach.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : ndarray of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corrected_std : float\n",
    "        Variance-corrected standard deviation of the set of differences.\n",
    "    \"\"\"\n",
    "    # kr = k times r, r times repeated k-fold crossvalidation,\n",
    "    # kr equals the number of times the model was evaluated\n",
    "    kr = len(differences)\n",
    "    corrected_var = np.var(differences, ddof=1) * (1 / kr + n_test / n_train)\n",
    "    corrected_std = np.sqrt(corrected_var)\n",
    "    return corrected_std\n",
    "\n",
    "\n",
    "def compute_corrected_ttest(differences, df, n_train, n_test):\n",
    "    \"\"\"Computes right-tailed paired t-test with corrected variance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    differences : array-like of shape (n_samples,)\n",
    "        Vector containing the differences in the score metrics of two models.\n",
    "    df : int\n",
    "        Degrees of freedom.\n",
    "    n_train : int\n",
    "        Number of samples in the training set.\n",
    "    n_test : int\n",
    "        Number of samples in the testing set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    t_stat : float\n",
    "        Variance-corrected t-statistic.\n",
    "    p_val : float\n",
    "        Variance-corrected p-value.\n",
    "    \"\"\"\n",
    "    mean = np.mean(differences)\n",
    "    std = corrected_std(differences, n_train, n_test)\n",
    "    t_stat = mean / std\n",
    "    p_val = t.sf(np.abs(t_stat), df)  # right-tailed t-test\n",
    "    return t_stat, p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1_scores = model_scores.iloc[0].values  # scores of the best model\n",
    "model_2_scores = model_scores.iloc[1].values  # scores of the second-best model\n",
    "\n",
    "differences = model_1_scores - model_2_scores\n",
    "\n",
    "n = differences.shape[0]  # number of test sets\n",
    "dof = n - 1\n",
    "\n",
    "t_stat, p_val = compute_corrected_ttest(\n",
    "    differences,\n",
    "    dof,\n",
    "    num_samples_train,\n",
    "    num_samples_test,\n",
    ")\n",
    "print(f\"Corrected t-statistic: {t_stat:.3f}\")\n",
    "print(f\"Corrected p-value: {p_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all models against the best model.\n",
    "best_model_scores = model_scores.iloc[0].values\n",
    "\n",
    "n_comparisons = model_scores.shape[0] - 1\n",
    "\n",
    "pairwise_t_test = []\n",
    "\n",
    "for model_i in range(1, len(model_scores)):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    differences = model_i_scores - best_model_scores\n",
    "    t_stat, p_val = compute_corrected_ttest(\n",
    "        differences,\n",
    "        dof,\n",
    "        num_samples_train,\n",
    "        num_samples_test,\n",
    "    )\n",
    "\n",
    "    # Implement Bonferroni correction\n",
    "    p_val *= n_comparisons\n",
    "\n",
    "    # Bonferroni can output p-values higher than 1\n",
    "    p_val = 1 if p_val > 1 else p_val\n",
    "\n",
    "    pairwise_t_test.append([\n",
    "        model_scores.index[0],\n",
    "        model_scores.index[model_i],\n",
    "        t_stat,\n",
    "        p_val,\n",
    "    ])\n",
    "\n",
    "pairwise_comp_df = pd.DataFrame(\n",
    "    pairwise_t_test,\n",
    "    columns=[\"model_1\", \"model_2\", \"t_stat\", \"p_val\"],\n",
    ").round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_comp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_i in range(1, len(model_scores)):\n",
    "    model_i_scores = model_scores.iloc[model_i].values\n",
    "    differences = model_i_scores - best_model_scores\n",
    "\n",
    "    name_i = model_scores.index[model_i]\n",
    "    name_best = model_scores.index[0]\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(model_i_scores, bins=30, alpha=0.5, label=name_i)\n",
    "    plt.hist(best_model_scores, bins=30, alpha=0.5, label=name_best)\n",
    "    plt.title(f'{name_i} vs {name_best}')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(differences, bins=30, alpha=0.5)\n",
    "    plt.title(f'differences {name_i} - {name_best}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
